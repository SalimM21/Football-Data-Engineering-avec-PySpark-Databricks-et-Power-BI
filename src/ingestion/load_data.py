# src/load_data.py
"""
Module : Chargement et pr√©paration initiale des donn√©es
-------------------------------------------------------
Ce script charge le fichier CSV des matchs de football, comprend les colonnes,
renomme celles utiles et supprime celles inutiles.

√âtapes :
    1. Charger le CSV dans un DataFrame PySpark
    2. Afficher la signification des colonnes (documentation)
    3. Renommer et supprimer les colonnes inutiles
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def load_csv_data(spark: SparkSession, csv_path: str):
    """
    Charge un fichier CSV dans un DataFrame PySpark avec gestion automatique du sch√©ma.

    Args:
        spark (SparkSession): La session Spark active.
        csv_path (str): Chemin du fichier CSV (dans Google Colab ou Databricks).

    Returns:
        DataFrame: DataFrame PySpark contenant les donn√©es charg√©es.
    """
    print("üìÇ Chargement du fichier CSV en cours...")

    df_raw = (
        spark.read
        .option("header", True)
        .option("inferSchema", True)
        .csv(csv_path)
    )

    print(f"‚úÖ Donn√©es charg√©es avec {df_raw.count()} lignes et {len(df_raw.columns)} colonnes.")
    return df_raw


def describe_columns():
    """
    Affiche la signification des colonnes du CSV brut.
    """
    print("\nüìò Description des colonnes brutes :")
    print("""
    - Match_ID  : Identifiant unique du match
    - Div        : Division ou niveau du championnat (ex : D1 = Bundesliga)
    - Season     : Ann√©e de d√©but de la saison (ex : 2005)
    - Date       : Date du match (AAAA-MM-JJ)
    - HomeTeam   : √âquipe √† domicile
    - AwayTeam   : √âquipe √† l‚Äôext√©rieur
    - FTHG       : Buts marqu√©s par l‚Äô√©quipe √† domicile
    - FTAG       : Buts marqu√©s par l‚Äô√©quipe √† l‚Äôext√©rieur
    - FTR        : R√©sultat final du match (H = Home win, A = Away win, D = Draw)
    """)


def clean_and_rename_columns(df_raw):
    """
    Renomme les colonnes cl√©s et supprime celles inutiles.

    Args:
        df_raw (DataFrame): DataFrame brut issu du CSV.

    Returns:
        DataFrame: DataFrame nettoy√© et renomm√©.
    """
    print("\nüßπ Nettoyage et renommage des colonnes...")

    df_cleaned = (
        df_raw
        .withColumnRenamed("FTHG", "HomeTeamGoals")
        .withColumnRenamed("FTAG", "AwayTeamGoals")
        .withColumnRenamed("FTR", "FinalResult")
        .select("Match_ID", "Div", "Season", "Date",
                "HomeTeam", "AwayTeam", "HomeTeamGoals", "AwayTeamGoals", "FinalResult")
    )

    print(f"‚úÖ Colonnes finales : {df_cleaned.columns}")
    return df_cleaned


def load_and_prepare_data(spark: SparkSession, csv_path: str):
    """
    Pipeline complet : chargement + description + nettoyage.

    Args:
        spark (SparkSession): Session Spark.
        csv_path (str): Chemin vers le CSV brut.

    Returns:
        DataFrame: DataFrame nettoy√© pr√™t pour les √©tapes suivantes.
    """
    df_raw = load_csv_data(spark, csv_path)
    describe_columns()
    df_cleaned = clean_and_rename_columns(df_raw)
    return df_cleaned


if __name__ == "__main__":
    # Exemple d'ex√©cution locale (Google Colab)
    spark = (
        SparkSession.builder
        .appName("FootballDataLoad")
        .master("local[*]")
        .getOrCreate()
    )

    csv_path = "/content/football_data.csv"  # üìå √† adapter selon ton chemin
    df_prepared = load_and_prepare_data(spark, csv_path)

    print("\nüìä Aper√ßu des donn√©es pr√©par√©es :")
    df_prepared.show(5)
