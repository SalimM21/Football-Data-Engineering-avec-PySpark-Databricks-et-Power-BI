# tests/test_load_data.py
"""
Tests unitaires pour le module load_data.py
-------------------------------------------
Ces tests valident le bon fonctionnement des fonctions de chargement et de pr√©paration
des donn√©es footballistiques avec PySpark.

üîç Ce que les tests v√©rifient :
    1. Le CSV est bien charg√© et contient les bonnes colonnes
    2. Les colonnes sont correctement renomm√©es
    3. Les colonnes inutiles ont √©t√© supprim√©es
"""

import pytest
from pyspark.sql import SparkSession
from src.load_data import load_csv_data, clean_and_rename_columns

# -------------------------------------------------------------------------
# üß∞ FIXTURE : Cr√©e une session Spark de test
# -------------------------------------------------------------------------
@pytest.fixture(scope="session")
def spark():
    spark = (
        SparkSession.builder
        .appName("FootballDataTest")
        .master("local[*]")
        .getOrCreate()
    )
    yield spark
    spark.stop()

# -------------------------------------------------------------------------
# üìò FIXTURE : Cr√©e un DataFrame PySpark d‚Äôexemple (simule le CSV)
# -------------------------------------------------------------------------
@pytest.fixture
def sample_df(spark):
    data = [
        (1, "D1", 2010, "2010-08-15", "Bayern", "Dortmund", 2, 1, "H"),
        (2, "D1", 2010, "2010-08-22", "Leverkusen", "Bayern", 0, 3, "A"),
        (3, "E0", 2010, "2010-09-10", "Chelsea", "Arsenal", 2, 2, "D"),
    ]
    columns = ["Match_ID", "Div", "Season", "Date", "HomeTeam", "AwayTeam", "FTHG", "FTAG", "FTR"]
    return spark.createDataFrame(data, columns)

# -------------------------------------------------------------------------
# ‚úÖ TEST 1 : V√©rifier que le DataFrame est bien charg√©
# -------------------------------------------------------------------------
def test_load_csv_data(spark, tmp_path):
    # Cr√©ation d‚Äôun petit CSV temporaire
    csv_file = tmp_path / "test_data.csv"
    with open(csv_file, "w") as f:
        f.write("Match_ID,Div,Season,Date,HomeTeam,AwayTeam,FTHG,FTAG,FTR\n")
        f.write("1,D1,2010,2010-08-15,Bayern,Dortmund,2,1,H\n")

    df = load_csv_data(spark, str(csv_file))
    assert df.count() == 1
    assert "HomeTeam" in df.columns

# -------------------------------------------------------------------------
# ‚úÖ TEST 2 : V√©rifier le renommage et nettoyage des colonnes
# -------------------------------------------------------------------------
def test_clean_and_rename_columns(sample_df):
    df_cleaned = clean_and_rename_columns(sample_df)

    expected_cols = [
        "Match_ID", "Div", "Season", "Date",
        "HomeTeam", "AwayTeam", "HomeTeamGoals", "AwayTeamGoals", "FinalResult"
    ]

    # V√©rifier que toutes les colonnes attendues sont pr√©sentes
    assert all(col in df_cleaned.columns for col in expected_cols)

    # V√©rifier que les anciennes colonnes ont disparu
    assert "FTHG" not in df_cleaned.columns
    assert "FTAG" not in df_cleaned.columns
    assert "FTR" not in df_cleaned.columns

    # V√©rifier que les valeurs ont √©t√© bien renomm√©es
    first_row = df_cleaned.collect()[0]
    assert isinstance(first_row.HomeTeamGoals, int)
    assert first_row.FinalResult in ["H", "A", "D"]

# -------------------------------------------------------------------------
# ‚úÖ TEST 3 : V√©rifier le nombre de colonnes finales
# -------------------------------------------------------------------------
def test_number_of_columns_after_cleaning(sample_df):
    df_cleaned = clean_and_rename_columns(sample_df)
    assert len(df_cleaned.columns) == 9  # Colonnes finales
